# Entropy Mechanism in Large Reasoning Models: A Survey

[![arXiv](https://img.shields.io/badge/arXiv-2XXX.XXXXX-b31b1b.svg)](link-to-paper)

This repository contains a curated list of papers on entropy mechanisms in Large
Reasoning Models (LRMs), covering both training and inference techniques.

![Timeline](timeline.png)
*Figure 2: Timeline of papers on entropy mechanisms in LRM within one year after the emergence of GRPO.*

## ðŸ“‹ Table of Contents

- [Training Methods](#training-methods)
  - [Sampling](#sampling)
  - [Advantage](#advantage)
  - [Clip](#clip)
  - [KL Penalty](#kl-penalty)
  - [Optimization](#optimization)
- [Inference Methods](#inference-methods)
  - [Entropyâ€‘Guided](#entropyâ€‘guided)
  - [Selfâ€‘Consistency](#selfâ€‘consistency)

---

## Training Methods

### Sampling

Methods that control which queries and trajectories are sampled during training.

| Paper | Institution | PDF | Code |
|-------|-------------|-----|------|
| **TreeRL: LLM Reinforcement Learning with Onâ€‘Policy Tree Search** | **Tsinghua University** | [arXiv](https://arxiv.org/pdf/2506.11902) | [GitHub](https://github.com/THUDM/TreeRL) |
| **First Return, Entropyâ€‘Eliciting Explore** | **ByteDance** | [arXiv](https://arxiv.org/pdf/2507.07017) | [HuggingFace](https://huggingface.co/FR3E-Bytedance) |
| **ETTRL: Balancing Exploration and Exploitation in LLM Testâ€‘Time RL** | **Kuaishou Technology** | [arXiv](https://arxiv.org/pdf/2508.11356) | â€“ |
| **Adaptive Dual Reasoner: Large Reasoning Models Can Think Efficiently by Hybrid Reasoning** | **Tencent Youtu Lab** | [arXiv](https://arxiv.org/pdf/2510.10207) | â€“ |
| **CURE: Criticalâ€‘Tokenâ€‘Guided Reâ€‘Concatenation for Entropyâ€‘Collapse Prevention** | **ByteDance** | [arXiv](https://arxiv.org/pdf/2508.11016) | [GitHub](https://github.com/bytedance/CURE) |
| **Agentic Reinforced Policy Optimization** | **Kuaishou Technology** | [arXiv](https://arxiv.org/pdf/2507.19849) | [GitHub](https://github.com/dongguanting/ARPO) |
| **Agentic Entropy-Balanced Policy Optimization** | **Kuaishou Technology** | [arXiv](https://arxiv.org/pdf/2510.14545) | [GitHub](https://github.com/dongguanting/ARPO) |
| **Arbitrary Entropy Policy Optimization: Entropy Is Controllable in Reinforcement Fineâ€‘Tuning** | **Nankai University** | [arXiv](https://arxiv.org/pdf/2510.08141) | [GitHub](https://github.com/597358816/AEPO) |


### Advantage

Methods that modify advantage calculations through entropy bonuses or coefficients.

| Paper | Institution | PDF | Code |
|-------|-------------|-----|------|
| **Reasoning with Exploration: An Entropy Perspective on RL for LLMs** | **Microsoft Research Asia** | [arXiv](https://arxiv.org/pdf/2506.14758) | â€“ |
| **No Prompt Left Behind: Exploiting Zeroâ€‘Variance Prompts in LLM RL via Entropyâ€‘Guided Advantage Shaping** | **Adobe Research** | [arXiv](https://arxiv.org/pdf/2509.21880) | â€“ |
| **Right Question is Already Half the Answer: Fully Unsupervised LLM Reasoning Incentivization** | **Tencent AI Lab** | [arXiv](https://arxiv.org/pdf/2504.05812) | [GitHub](https://github.com/QingyangZhang/EMPO) |
| **Decomposing the Entropyâ€‘Performance Exchange: The Missing Keys to Unlocking Effective RL** | **Renmin University of China** | [arXiv](https://arxiv.org/pdf/2508.02260) | â€“ |
| **PEAR: Phase Entropy Aware Reward for Efficient Reasoning** | **Singapore University of Technology and Design** | [arXiv](https://arxiv.org/pdf/2510.08026) | [GitHub](https://github.com/iNLP-Lab/PEAR) |
| **Harnessing Uncertainty: Entropyâ€‘Modulated Policy Gradients for Longâ€‘Horizon LLM Agents** | **ByteDance** | [arXiv](https://arxiv.org/pdf/2509.09265) | â€“ |
| **The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning** | **University of Illinois at Urbanaâ€‘Champaign** | [arXiv](https://arxiv.org/pdf/2505.15134) | [GitHub](https://github.com/shivamag125/EM_PT) |
| **SEEDâ€‘GRPO: Semantic Entropy Enhanced GRPO for Uncertaintyâ€‘Aware Policy Optimization** | **Zhejiang University** | [arXiv](https://arxiv.org/pdf/2505.12346) | â€“ |
| **EDGEâ€‘GRPO: Entropyâ€‘Driven GRPO with Guided Error Correction for Advantage Diversity** | **Beihang University** | [arXiv](https://arxiv.org/pdf/2507.21848) | [GitHub](https://github.com/ZhangXJ199/EDGE-GRPO) |
| **Unlocking Exploration in RLVR: Uncertaintyâ€‘Aware Advantage Shaping for Deeper Reasoning** | **Kuaishou Technology** | [arXiv](https://arxiv.org/pdf/2510.10649) | [GitHub](https://github.com/xvolcano02/UCAS) |
| **Pinpointing Crucial Steps: Attributionâ€‘Based Credit Assignment for Verifiable RL** | **Taikang Insurance Group Inc.** | [arXiv](https://arxiv.org/pdf/2510.08899) | â€“ |
| **Conditional Advantage Estimation for RL in Large Reasoning Models** | **Shanghai Jiao Tong University** | [arXiv](https://arxiv.org/pdf/2509.23962) | [GitHub](https://github.com/biuboomc/CANON) |
| **Quantile Advantage Estimation for Entropyâ€‘Safe Reasoning** | **University of Science and Technology of China** | [arXiv](https://arxiv.org/pdf/2509.22611) | [GitHub](https://github.com/junkangwu/QAE) |
| **AdaThinkâ€‘Med: Medical Adaptive Thinking with Uncertaintyâ€‘Guided Length Calibration** | **Shanghai Jiao Tong University** | [arXiv](https://arxiv.org/pdf/2509.24560) | [GitHub](https://github.com/shaohao011/AdaThinkMed) |
| **Beyond High-Entropy Exploration: Correctness-Aware Low-Entropy Segment-Based Advantage Shaping for Reasoning LLMs** | **Beijing University of Posts and Telecommunications** | [arXiv](https://arxiv.org/pdf/2512.00908) | - |

### Clip

Methods that adjust clipping mechanisms to control policy updates.

| Paper | Institution | PDF | Code |
|-------|-------------|-----|------|
| **DAPO: An Openâ€‘Source LLM RL System at Scale** | **ByteDance** | [arXiv](https://arxiv.org/pdf/2503.14476) | [GitHub](https://github.com/BytedTsinghua-SIA/DAPO) |
| **DLER: Doing Length pEnalty Right â€“ Incentivizing More Intelligence per Token via RL** | **NVIDIA** | [arXiv](https://arxiv.org/pdf/2510.15110) | [GitHub](https://github.com/NVlabs/DLER) |
| **Toward Better EHR Reasoning in LLMs: RL with Expert Attention Guidance** | **Peking University** | [arXiv](https://arxiv.org/pdf/2508.13579) | [GitHub](https://github.com/devilran6/EAG-RL) |
| **Clipâ€‘Low Increases Entropy and Clipâ€‘High Decreases Entropy in RL of Large Language Models** | **University of California, Los Angeles** | [arXiv](https://arxiv.org/pdf/2509.26114) | â€“ |
| **BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping** | **Fudan University** | [arXiv](https://arxiv.org/pdf/2510.18927) | [GitHub](https://github.com/WooooDyy/BAPO)  |
| **Agentic Entropyâ€‘Balanced Policy Optimization** | **Kuaishou Technology** | [arXiv](https://arxiv.org/pdf/2510.14545) | [GitHub](https://github.com/dongguanting/ARPO) |
| **Stabilizing Knowledge, Promoting Reasoning: Dualâ€‘Token Constraints for RLVR** | **Kuaishou Technology** | [arXiv](https://arxiv.org/pdf/2507.15778) | [GitHub](https://github.com/wizard-III/ArcherCodeR) |
| **DCPO: Dynamic Clipping Policy Optimization** | **Baichuan.inc** | [arXiv](https://arxiv.org/pdf/2509.02333) | [GitHub](https://github.com/lime-RL/DCPO) |
| **ASPO: Asymmetric Importance Sampling Policy Optimization** | **Kuaishou Technology** | [arXiv](https://arxiv.org/pdf/2510.06062) | [GitHub](https://github.com/wizard-III/Archer2.0) |
| **CEâ€‘GPPO: Controlling Entropy via Gradientâ€‘Preserving Clipping Policy Optimization in RL** | **Kuaishou Technology** | [arXiv](https://arxiv.org/pdf/2509.20712) | â€“ |
| **The Entropy Mechanism of RL for Reasoning Language Models** | **Shanghai AI Laboratory** | [arXiv](https://arxiv.org/pdf/2505.22617) | [GitHub](https://github.com/PRIME-RL/Entropy-Mechanism-of-RL) |
| **Prosperity before Collapse: How Far Can Offâ€‘Policy RL Reach with Stale Data on LLMs?** | **Meta AI** | [arXiv](https://arxiv.org/pdf/2510.01161) | [GitHub](https://github.com/Infini-AI-Lab/M2PO) |
| **ESPO: Entropy Importance Sampling Policy Optimization** | **Meta AI** | [arXiv](https://www.arxiv.org/pdf/2512.00499) | - |
| **Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning** | **Kuaishou Technology** | [arXiv](https://arxiv.org/pdf/2512.05591) | - |

### KL Penalty

Methods that adjust KL divergence constraints to regulate exploration.

| Paper | Institution | PDF | Code |
|-------|-------------|-----|------|
| **Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model** | **StepFun** | [arXiv](https://arxiv.org/pdf/2503.24290) | [GitHub](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero) |
| **DAPO: An Openâ€‘Source LLM RL System at Scale** | **ByteDance** | [arXiv](https://arxiv.org/pdf/2503.14476) | [GitHub](https://github.com/BytedTsinghua-SIA/DAPO) |
| **Agentic Entropyâ€‘Balanced Policy Optimization** | **Kuaishou Technology** | [arXiv](https://arxiv.org/pdf/2510.14545) | [GitHub](https://github.com/dongguanting/ARPO) |
| **ASPO: Asymmetric Importance Sampling Policy Optimization** | **Kuaishou Technology** | [arXiv](https://arxiv.org/pdf/2510.06062) | [GitHub](https://github.com/wizard-III/Archer2.0) |
| **ProRL: Prolonged RL Expands Reasoning Boundaries** | **NVIDIA** | [arXiv](https://arxiv.org/pdf/2505.24864) | â€“ |
| **The Entropy Mechanism of RL for Reasoning Language Models** | **Shanghai AI Laboratory** | [arXiv](https://arxiv.org/pdf/2505.22617) | [GitHub](https://github.com/PRIME-RL/Entropy-Mechanism-of-RL) |
| **APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy Optimization** | **Zhejiang University** | [arXiv](https://arxiv.org/pdf/2506.21655) | [GitHub](https://github.com/Indolent-Kawhi/View-R1) |
| **Stabilizing Knowledge, Promoting Reasoning: Dualâ€‘Token Constraints for RLVR** | **Kuaishou Technology** | [arXiv](https://arxiv.org/pdf/2507.15778) | [GitHub](https://github.com/wizard-III/ArcherCodeR) |
| **RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning** | **Northwestern University** | [arXiv](https://arxiv.org/pdf/2507.14843) | [GitHub](https://github.com/mll-lab-nu/RAGEN) |
| **Efficient Reinforcement Learning with Semantic and Token Entropy for LLM Reasoning** | **Nanjing University** | [arXiv](https://arxiv.org/pdf/2512.04359) | - |


### Optimization

Methods that modify the optimization objective or reâ€‘weight losses.

| Paper | Institution | PDF | Code |
|-------|-------------|-----|------|
| **Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation** | **Tencent AI Lab** | [arXiv](https://arxiv.org/pdf/2509.15194) | â€“ |
| **Rediscovering Entropy Regularization: Adaptive Coefficient Unlocks Its Potential for LLM RL** | **StepFun Inc.** | [arXiv](https://arxiv.org/pdf/2510.10959) | â€“ |
| **On Entropy Control in LLMâ€‘RL Algorithms** | **Ant Group** | [arXiv](https://arxiv.org/pdf/2509.03493) | â€“ |
| **What Makes Reasoning Invalid: Echo Reflection Mitigation for LLMs** | **Salesforce AI Research** | [arXiv](https://arxiv.org/pdf/2511.06380) | â€“ |
| **EPO: Entropyâ€‘Regularized Policy Optimization for LLM Agents RL** | **Adobe** | [arXiv](https://arxiv.org/pdf/2509.22576) | [GitHub](https://github.com/WujiangXu/EPO) |
| **Oneâ€‘shot Entropy Minimization** | **Ubiquant** | [arXiv](https://arxiv.org/pdf/2505.20282) | [GitHub](https://github.com/zitian-gao/one-shot-em) |
| **FlowRL: Matching Reward Distributions for LLM Reasoning** | **Shanghai AI Laboratory** | [arXiv](https://arxiv.org/pdf/2509.15207) | â€“ |
| **Beyond the 80/20 Rule: Highâ€‘Entropy Minority Tokens Drive Effective RL for LLM Reasoning** | **Alibaba Inc.** | [arXiv](https://arxiv.org/pdf/2506.01939) | â€“ |
| **Efficient Multiâ€‘Turn RL for GUI Agents via Decoupled Training and Adaptive Data Curation** | **DataCanvas** | [arXiv](https://arxiv.org/pdf/2509.23866) | â€“ |
| **Learning More with Less: A Dynamic Dualâ€‘Level Downâ€‘Sampling Framework for Efficient Policy Optimization** | **WeChat (Tencent)** | [arXiv](https://arxiv.org/pdf/2509.22115) | â€“ |
| **UloRL: An Ultraâ€‘Long Output RL Approach for Advancing LLM Reasoning Abilities** | **Tencent Hunyuan Team** | [arXiv](https://arxiv.org/pdf/2507.19766) | [GitHub](https://github.com/liushulinle/ULORL) |
| **Revisiting Entropy in Reinforcement Learning for Large Reasoning Models** | **Tianjin University** | [arXiv](https://arxiv.org/pdf/2511.05993) | - |
| **SIMKO: Simple Pass@K Policy Optimization** | **Westlake University** | [arXiv](https://arxiv.org/pdf/2510.14807) | [GitHub](https://github.com/CLR-Lab/SimKO) |
| **The Entropy Mechanism of RL for Reasoning Language Models** | **Shanghai AI Laboratory** | [arXiv](https://arxiv.org/pdf/2505.22617) | [GitHub](https://github.com/PRIME-RL/Entropy-Mechanism-of-RL) |
| **Rethinking Entropy Interventions in RLVR: An Entropy Change Perspective** | **Tencent** | [arXiv](https://arxiv.org/pdf/2510.10150) | [GitHub](https://github.com/zz-haooo/STEER) |
| **EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control** | **Tencent** | [arXiv](https://arxiv.org/pdf/2511.15248) | [GitHub](https://github.com/yk7333/EntroPIC) |
---

## Inference Methods

### Entropyâ€‘Guided

Methods that use entropy to guide inference decisions and control reasoning processes.

| Paper | Institution | PDF | Code |
|-------|-------------|-----|------|
| **Detecting Hallucinations in Large Language Models Using Semantic Entropy** | **University of Oxford** | [Nature](https://arxiv.org/pdf/2405.19648) | â€“ |
| **Trace Length Is a Simple Uncertainty Signal in Reasoning Models** | **Apple** | [arXiv](https://arxiv.org/pdf/2510.10409) | â€“ |
| **Entropyâ€‘Guided Loop: Achieving Reasoning through Uncertaintyâ€‘Aware Generation** | **Monostate** | [arXiv](https://arxiv.org/pdf/2509.00079) | [GitHub](https://github.com/andrewmonostate/paper-entropy-loop) |
| **EAT: Entropy After `<Think>` for Reasoning Model Early Exiting** | **Netflix Research** | [arXiv](https://arxiv.org/pdf/2509.26522) | [GitHub](https://github.com/xidulu/EAT) |
| **DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching** | **Johns Hopkins University** | [arXiv](https://arxiv.org/pdf/2511.00640) | [GitHub](https://github.com/ZichengXu/Decoding-Tree-Sketching) |
| **EAGER: Entropyâ€‘Aware Generation for Adaptive Inferenceâ€‘Time Scaling** | **Cohere** | [arXiv](https://arxiv.org/pdf/2510.11170) | [GitHub](https://github.com/DanielSc4/EAGer) |
| **Adaptive Termination for Multiâ€‘Round Parallel Reasoning: A Universal Semantic Entropyâ€‘Guided Framework** | **Tencent Hunyuan T1 Team** | [arXiv](https://arxiv.org/pdf/2507.06829) | â€“ |

### Selfâ€‘Consistency

Methods that leverage entropy for answer aggregation and consistency enhancement.

| Paper | Institution | PDF | Code |
|-------|-------------|-----|------|
| **Selective Expert Guidance for Effective and Diverse Exploration in RL of LLMs** | **Ant Group** | [arXiv](https://arxiv.org/pdf/2510.04140) | [GitHub](https://github.com/Jiangzs1028/MENTOR) |
| **DualResearch: Entropyâ€‘Gated Dualâ€‘Graph Retrieval for Answer Reconstruction** | **Shanghai Artificial Intelligence Laboratory** | [arXiv](https://arxiv.org/pdf/2510.08959) | â€“ |
| **The Sequential Edge: Inverseâ€‘Entropy Voting Beats Parallel Selfâ€‘Consistency at Matched Compute** | **Lossfunk** | [arXiv](https://arxiv.org/pdf/2511.02309) | â€“ |

---

## Citation

If you find this repository useful, please consider citing:

```bibtex
@article{entropy_lrm_survey2025,
  title={A Survey about Entropy Mechanism in Large Reasoning Models},
  author={Anonymous},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2025}
}
